{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U torch torchvision;","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tqdm.autonotebook import tqdm, trange\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\n\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:21:57.144157Z","iopub.execute_input":"2021-11-20T12:21:57.144528Z","iopub.status.idle":"2021-11-20T12:21:57.981690Z","shell.execute_reply.started":"2021-11-20T12:21:57.144437Z","shell.execute_reply":"2021-11-20T12:21:57.980887Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:21:59.447320Z","iopub.execute_input":"2021-11-20T12:21:59.447587Z","iopub.status.idle":"2021-11-20T12:21:59.451690Z","shell.execute_reply.started":"2021-11-20T12:21:59.447557Z","shell.execute_reply":"2021-11-20T12:21:59.450875Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import random \nSEED = 7\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:01.156723Z","iopub.execute_input":"2021-11-20T12:22:01.156992Z","iopub.status.idle":"2021-11-20T12:22:01.165112Z","shell.execute_reply.started":"2021-11-20T12:22:01.156957Z","shell.execute_reply":"2021-11-20T12:22:01.164325Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"TRAIN_DIR = Path('../input/journey-springfield/train')\nTEST_DIR = Path('../input/journey-springfield/testset')\nDATA_MODES = ['train', 'val', 'test']\nRESCALE_SIZE = 224\nDEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nCUDA_LAUNCH_BLOCKING=1\nBATCH_SIZE=128\n\nprint(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:02.642213Z","iopub.execute_input":"2021-11-20T12:22:02.642519Z","iopub.status.idle":"2021-11-20T12:22:02.671931Z","shell.execute_reply.started":"2021-11-20T12:22:02.642490Z","shell.execute_reply":"2021-11-20T12:22:02.671216Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class SimpsonsDataset(Dataset):\n\n    def __init__(self, files, mode):\n        super().__init__()\n        # список файлов для загрузки\n        self.files = sorted(files)\n        # режим работы\n        self.mode = mode\n\n        if self.mode not in DATA_MODES:\n            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n            raise NameError\n\n        self.len_ = len(self.files)\n     \n        self.label_encoder = LabelEncoder()\n\n        if self.mode != 'test':\n            self.labels = [path.parent.name for path in self.files]\n            self.label_encoder.fit(self.labels)\n\n            with open('label_encoder.pkl', 'wb') as le_dump_file:\n                  pickle.dump(self.label_encoder, le_dump_file)\n                      \n    def __len__(self):\n        return self.len_\n      \n    def load_sample(self, file):\n        image = Image.open(file)\n        image.load()\n        return image\n  \n    def __getitem__(self, index):\n        # для преобразования изображений в тензоры PyTorch и нормализации входа\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n        x = self.load_sample(self.files[index])\n        x = self._prepare_sample(x)\n        x = np.array(x / 255, dtype='float32')\n        x = transform(x)\n        if self.mode == 'test':\n            return x\n        else:\n            label = self.labels[index]\n            label_id = self.label_encoder.transform([label])\n            y = label_id.item()\n            return x, y\n        \n    def _prepare_sample(self, image):\n        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n        return np.array(image)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:13.504962Z","iopub.execute_input":"2021-11-20T12:22:13.505799Z","iopub.status.idle":"2021-11-20T12:22:13.518222Z","shell.execute_reply.started":"2021-11-20T12:22:13.505752Z","shell.execute_reply":"2021-11-20T12:22:13.517169Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_val_files = sorted(list(TRAIN_DIR.rglob('*.jpg')))\ntest_files = sorted(list(TEST_DIR.rglob('*.jpg')))\n\nfrom sklearn.model_selection import train_test_split\n\ntrain_val_labels = [path.parent.name for path in train_val_files]\ntrain_files, val_files = train_test_split(train_val_files, test_size=0.25, \\\n                                          stratify=train_val_labels)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:20.945484Z","iopub.execute_input":"2021-11-20T12:22:20.946347Z","iopub.status.idle":"2021-11-20T12:22:22.239778Z","shell.execute_reply.started":"2021-11-20T12:22:20.946290Z","shell.execute_reply":"2021-11-20T12:22:22.239024Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"n_classes = len(np.unique(train_val_labels))\nn_classes","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:23.601796Z","iopub.execute_input":"2021-11-20T12:22:23.602322Z","iopub.status.idle":"2021-11-20T12:22:23.623317Z","shell.execute_reply.started":"2021-11-20T12:22:23.602284Z","shell.execute_reply":"2021-11-20T12:22:23.622350Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_labels = [path.parent.name for path in train_files] # классы train\nval_labels = [path.parent.name for path in val_files]     # классы val","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:25.071146Z","iopub.execute_input":"2021-11-20T12:22:25.071762Z","iopub.status.idle":"2021-11-20T12:22:25.117808Z","shell.execute_reply.started":"2021-11-20T12:22:25.071723Z","shell.execute_reply":"2021-11-20T12:22:25.117054Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def create_dct_path_labels(train_files, train_labels):\n    dct_simpsons = {}\n    for label_i in np.unique(train_labels).tolist():\n        dct_simpsons[label_i] = []\n\n    for path_i, label_i in zip(train_files, train_labels):\n        dct_simpsons[label_i].append(path_i)\n\n    return dct_simpsons\n\ndef print_dct(dct_simpsons):\n    for key in dct_simpsons:\n        print(f\"{key}\\t{dct_simpsons[key]}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:26.716547Z","iopub.execute_input":"2021-11-20T12:22:26.717116Z","iopub.status.idle":"2021-11-20T12:22:26.724905Z","shell.execute_reply.started":"2021-11-20T12:22:26.717076Z","shell.execute_reply":"2021-11-20T12:22:26.723907Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"dct_path_train = create_dct_path_labels(train_files, train_labels)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:28.789794Z","iopub.execute_input":"2021-11-20T12:22:28.790267Z","iopub.status.idle":"2021-11-20T12:22:28.807389Z","shell.execute_reply.started":"2021-11-20T12:22:28.790214Z","shell.execute_reply":"2021-11-20T12:22:28.806678Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Дополним картинки классов у которых менее 100 картинок, до 100 картинок в классе\nfor person in dct_path_train:\n    if len(dct_path_train[person]) < 100:\n        dct_path_train[person] = dct_path_train[person] * (100 // len(dct_path_train[person]))\n        dct_path_train[person].extend(dct_path_train[person][:100 - len(dct_path_train[person])])\n\n# Проверим что получилось \nfor person in dct_path_train:\n    print(f\"{person}\\t{len(dct_path_train[person])}\")","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:30.750437Z","iopub.execute_input":"2021-11-20T12:22:30.751256Z","iopub.status.idle":"2021-11-20T12:22:30.761976Z","shell.execute_reply.started":"2021-11-20T12:22:30.751200Z","shell.execute_reply":"2021-11-20T12:22:30.761080Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def create_dct_from_labels(train_val_labels):\n    \"\"\"Функция создает из list train_val_labels, содержащего метки классов\n    соответсвующим картинкам из выборки, словарь dict с ключами соответсвующими\n    названиям классов, и значениями, соответвующими колчеству этих классов в \n    list train_val_labels\"\"\"\n    dct_simpsons = {}\n    for label_i in np.unique(train_val_labels).tolist():\n        dct_simpsons.update({label_i:train_val_labels.count(label_i)})\n\n    return dct_simpsons\n\nnew_train_files = []\n\nfor person in dct_path_train:\n    new_train_files.extend(dct_path_train[person])\n\nnew_train_label = [path.parent.name for path in new_train_files]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:33.544767Z","iopub.execute_input":"2021-11-20T12:22:33.545077Z","iopub.status.idle":"2021-11-20T12:22:33.608573Z","shell.execute_reply.started":"2021-11-20T12:22:33.545044Z","shell.execute_reply":"2021-11-20T12:22:33.607895Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"val_dataset = SimpsonsDataset(val_files, mode='val')\nnew_train_dataset = SimpsonsDataset(new_train_files, mode='train')","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:37.791275Z","iopub.execute_input":"2021-11-20T12:22:37.791767Z","iopub.status.idle":"2021-11-20T12:22:37.972582Z","shell.execute_reply.started":"2021-11-20T12:22:37.791729Z","shell.execute_reply":"2021-11-20T12:22:37.971802Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def fit_epoch(model, train_dataloader, criterion, optimizer):\n    running_loss = 0.0\n    running_corrects = 0\n    processed_data = 0\n  \n    for inputs, labels in train_dataloader:\n        inputs = inputs.to(DEVICE) #перенос тензоров на видеокарту\n        labels = labels.to(DEVICE)\n        optimizer.zero_grad() #обнуляем градиенты, чтобы они не накапливались\n\n        outputs = model(inputs) #прогоняем данные из трейнлоадера чеез нашу сеть (модель). интересно, почему не model.forward(inputs)?\n        loss = criterion(outputs, labels) #считаем лосс\n        loss.backward()\n        optimizer.step()\n        preds = torch.argmax(outputs, 1) #все, что больше 1, приводим к 1\n        running_loss += loss.item() * inputs.size(0) #непонятно, как работает .size(0)\n        running_corrects += torch.sum(preds == labels.data) #если предсказание совпадает с ответом, увеличиваем на 1 running_corrects\n        processed_data += inputs.size(0) \n              \n    train_loss = running_loss / processed_data #потери на трейне\n    train_acc = running_corrects.cpu().numpy() / processed_data #точность на трейне\n    return train_loss, train_acc","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:39.741785Z","iopub.execute_input":"2021-11-20T12:22:39.742062Z","iopub.status.idle":"2021-11-20T12:22:39.749280Z","shell.execute_reply.started":"2021-11-20T12:22:39.742031Z","shell.execute_reply":"2021-11-20T12:22:39.748522Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def eval_epoch(model, val_dataloader, criterion):\n    model.eval()\n    running_loss = 0.0\n    running_corrects = 0\n    processed_size = 0\n\n    for inputs, labels in val_dataloader:\n        inputs = inputs.to(DEVICE)\n        labels = labels.to(DEVICE)\n\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            preds = torch.argmax(outputs, 1)\n\n        running_loss += loss.item() * inputs.size(0)\n        running_corrects += torch.sum(preds == labels.data)\n        processed_size += inputs.size(0)\n    val_loss = running_loss / processed_size\n    val_acc = running_corrects.double() / processed_size\n    return val_loss, val_acc","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:41.976776Z","iopub.execute_input":"2021-11-20T12:22:41.977059Z","iopub.status.idle":"2021-11-20T12:22:41.984457Z","shell.execute_reply.started":"2021-11-20T12:22:41.977029Z","shell.execute_reply":"2021-11-20T12:22:41.983453Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def train(train_files, val_files, model, epochs, batch_size):\n    train_dataloader = DataLoader(new_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    history = []\n    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n\n    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n        opt = torch.optim.Adam(model.parameters())\n        criterion = nn.CrossEntropyLoss()\n\n        for epoch in range(epochs):\n            train_loss, train_acc = fit_epoch(model, train_dataloader, criterion, opt)\n            print(\"loss\", train_loss)\n            \n            val_loss, val_acc = eval_epoch(model, val_dataloader, criterion)\n            history.append((train_loss, train_acc, val_loss, val_acc))\n            \n            pbar_outer.update(1)\n            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n            \n    return history","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:44.203699Z","iopub.execute_input":"2021-11-20T12:22:44.204258Z","iopub.status.idle":"2021-11-20T12:22:44.216941Z","shell.execute_reply.started":"2021-11-20T12:22:44.204206Z","shell.execute_reply":"2021-11-20T12:22:44.216251Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"model = models.alexnet(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:47.225702Z","iopub.execute_input":"2021-11-20T12:22:47.226475Z","iopub.status.idle":"2021-11-20T12:22:47.876698Z","shell.execute_reply.started":"2021-11-20T12:22:47.226422Z","shell.execute_reply":"2021-11-20T12:22:47.875716Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for param in model.parameters():\n    param.requires_grad = False\n\nnum_features = 9216\nmodel.classifier = nn.Linear(num_features, n_classes)\nmodel = model.to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:49.864807Z","iopub.execute_input":"2021-11-20T12:22:49.865683Z","iopub.status.idle":"2021-11-20T12:22:51.736020Z","shell.execute_reply.started":"2021-11-20T12:22:49.865615Z","shell.execute_reply":"2021-11-20T12:22:51.735292Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"history = train(new_train_dataset, val_dataset, model=model, epochs=8, batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:22:54.544353Z","iopub.execute_input":"2021-11-20T12:22:54.545341Z","iopub.status.idle":"2021-11-20T12:44:13.195256Z","shell.execute_reply.started":"2021-11-20T12:22:54.545300Z","shell.execute_reply":"2021-11-20T12:44:13.194505Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"loss, acc, val_loss, val_acc = zip(*history)","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:44:19.259408Z","iopub.execute_input":"2021-11-20T12:44:19.259915Z","iopub.status.idle":"2021-11-20T12:44:19.263507Z","shell.execute_reply.started":"2021-11-20T12:44:19.259877Z","shell.execute_reply":"2021-11-20T12:44:19.262739Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'Simpsons_AlexNet.pth')\nmodel.load_state_dict(torch.load('Simpsons_AlexNet.pth'))","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:44:21.848685Z","iopub.execute_input":"2021-11-20T12:44:21.849160Z","iopub.status.idle":"2021-11-20T12:44:21.888954Z","shell.execute_reply.started":"2021-11-20T12:44:21.849116Z","shell.execute_reply":"2021-11-20T12:44:21.888185Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(loss, label=\"train_loss\")\nplt.plot(val_loss, label=\"val_loss\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"loss\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:44:25.528759Z","iopub.execute_input":"2021-11-20T12:44:25.529357Z","iopub.status.idle":"2021-11-20T12:44:25.759305Z","shell.execute_reply.started":"2021-11-20T12:44:25.529319Z","shell.execute_reply":"2021-11-20T12:44:25.758586Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(acc, label=\"train_acc\")\nplt.plot(val_acc, label=\"val_acc\")\nplt.legend(loc='best')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"acc\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:44:28.145884Z","iopub.execute_input":"2021-11-20T12:44:28.146140Z","iopub.status.idle":"2021-11-20T12:44:28.373674Z","shell.execute_reply.started":"2021-11-20T12:44:28.146111Z","shell.execute_reply":"2021-11-20T12:44:28.372955Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def predict(model, test_loader):\n    with torch.no_grad(): #без градиентов\n        logits = []\n    \n        for inputs in test_loader:\n            inputs = inputs.to(DEVICE)\n            model.eval()\n            outputs = model(inputs).cpu()\n            logits.append(outputs)\n            \n    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n    return probs","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:44:32.323823Z","iopub.execute_input":"2021-11-20T12:44:32.324082Z","iopub.status.idle":"2021-11-20T12:44:32.330258Z","shell.execute_reply.started":"2021-11-20T12:44:32.324053Z","shell.execute_reply":"2021-11-20T12:44:32.329273Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def predict_one_sample(model, inputs, device=DEVICE):\n    \"\"\"Предсказание, для одной картинки\"\"\"\n    with torch.no_grad():\n        inputs = inputs.to(device)\n        model.eval()\n        logit = model(inputs).cpu()\n        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n    return probs","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:44:36.178062Z","iopub.execute_input":"2021-11-20T12:44:36.178342Z","iopub.status.idle":"2021-11-20T12:44:36.184158Z","shell.execute_reply.started":"2021-11-20T12:44:36.178309Z","shell.execute_reply":"2021-11-20T12:44:36.183158Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"random_characters = int(np.random.uniform(0,1000))\nex_img, true_label = val_dataset[random_characters]\nprobs_im = predict_one_sample(model, ex_img.unsqueeze(0))\n\nidxs = list(map(int, np.random.uniform(0,1000, 20)))\nimgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n\nprobs_ims = predict(model, imgs)\n\nlabel_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n\ny_pred = np.argmax(probs_ims,-1)\n\nactual_labels = [val_dataset[id][1] for id in idxs]\n\npreds_class = [label_encoder.classes_[i] for i in y_pred]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:44:39.202363Z","iopub.execute_input":"2021-11-20T12:44:39.202917Z","iopub.status.idle":"2021-11-20T12:44:39.529891Z","shell.execute_reply.started":"2021-11-20T12:44:39.202877Z","shell.execute_reply":"2021-11-20T12:44:39.529125Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test_dataset = SimpsonsDataset(test_files, mode=\"test\")\ntest_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\nprobs = predict(model, test_loader) #матрица с вероятностями для каждого класса\n\npreds = label_encoder.inverse_transform(np.argmax(probs, axis=1)) #вектор в наибольшими вероятностями\ntest_filenames = [path.name for path in test_dataset.files]","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:44:45.563762Z","iopub.execute_input":"2021-11-20T12:44:45.564023Z","iopub.status.idle":"2021-11-20T12:44:53.365799Z","shell.execute_reply.started":"2021-11-20T12:44:45.563994Z","shell.execute_reply":"2021-11-20T12:44:53.365084Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# ДОБАВЛЕНО: создание сабмита\nimport pandas as pd\ndf = pd.DataFrame()\ndf['Id'] = test_filenames\ndf['Expected'] = preds\ndf.to_csv(Path('./submission.csv'), index=False)\n# файл появится у вас на гугл диске","metadata":{"execution":{"iopub.status.busy":"2021-11-20T12:44:56.572846Z","iopub.execute_input":"2021-11-20T12:44:56.573578Z","iopub.status.idle":"2021-11-20T12:44:56.594499Z","shell.execute_reply.started":"2021-11-20T12:44:56.573541Z","shell.execute_reply":"2021-11-20T12:44:56.593641Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}